# Local-RAG-Projects
I am a 16 year old working on some Local Ollama RAG Project mostly with deepseek-r1. I would really appreciate if you could check out my work, improve it and also help me on localhost web designs for my projects! Thank You!

## MefTok2.0
The first app is for IB Diploma Programme's Theory of Knowledge lesson. The pdf named a13.pdf is the dataset I provided for deepseek-r1.:1.5b (The reason I use 1.5b is because of my computer's weak Ram so you can try out 8b or 14b as well. Also you can use a bigger pdf but the answer generation will take longer due to chunk searching.) For that code I also added the pip installs and imports necessary to avoid any confusion. I worked on a pyhton jupyter notebook on Visual Studio Code. The pip installations need to be on specific versions which I added on requirements.txt . Also you need to install Ollama to use these Local RAG Ai projects.

#### 1-Use a command bar or windows powershell while working with Ollama. 
#### 2-You need to write in ollama pull deepseek-r1:1.5b to download the LLM you will use (you can try other deepseek-r1 models with more parameters.) 
#### 3-tpye in ollama serve to start ollama so the QA chain is ran through the LLM of your choice.
#### 4-Important issue I come accross is ollama keeps running on the background and you cannot serve it again when you open your PC. You need to go to your command bar or windows powershell and type tasklist | findstr Ollamna to locate the running server. Than type in taskkil /F /PIF - - - - (the 4 number PID written on screen). Than after terminating those tasks you can go ahead and type in Ollama serve

### Explaining Code Blocks
#### 1- The Installs block includes external installations you need to use to get the code working. Langchain is an opensource framework which includes helpful tools and agents for local Large Language Model(LLM projects. Faiss is the database we will use to store our embeddings as vectors, you can also use ChromaDB if you wish which is better in SelfQuery retrieval. Pypdf is to import our pdf files so they afterwards get divided into chunks an become embedded with ollama embedding models. protobuf and Iphyton is to set the enviroment on Jupyter.
#### 2- Imports block is to import the packages and tools we will use from the pip installs we had done. I imported multiple databases, pdf converters and textsplitters so you can use the one that you think works best. You can also make commitments and update me on more tools you found or notify me on which ones suit this RAG best.
#### 3- The third block loads the pdf (There are a lot of comverters especially on Unstructured such as .csv or word files.) and then splits it into chunks. The RAG searchers for the chunk which it thinks includes the answer and then answers w√ºth the information from that chunk. I use RecursiveCharacterTextSplitter which understand paragraphs and text structures. You can increase the chunksize if you want a more detailed answers which will decrease the chunks numbers as well. Chunk overlap is the point in the pdf where two chunks meet. This is crucial since you don't randomly want it to start from the middle of a sentence or paragraph and be able to use info from previous and next chunk as well. I also made it print the first chunk so you can check if the text was readable by the loader.
#### 4-This block is pretty simple. It is to create embeddings which will be proccesed by the LLM. I used nomic-embed-text but again there are other embedding styles as well which you can try out and update me on which one could be better. When running this block make sure that you have Ollama server running in the background or it will not work. Also this may take couple minutes depending on your chunk number.
#### 5- The fifth block is to set the basics of the app. We set our retrieval system which you can also do with MultiQuery retireval or SelfQuery retrieval (I would appreciate if you try other retrievals and LLms for this RAG as diffrent branches.) Searching kwargs is how many chunks it will search and retrieve. Then we choose our LLM, you can set the temperature depending on your needs. 0 gives factual answers and 1 gives more creative answers which is better for coming up with ideas etc. Than I created a simple question answer chain which is a needed code for this app to work. I don't have much information on how you could customize the QA chain so I am open to suggestions.
#### 6- This is the block where you type in your question and wait for the answer in the part below the code block. For this part I want help on Verbose. Although I add Verbose = True I still see the thinking proccess of deepseek-r1 as well as the answer. How can I disable this?


Thank you for your contributions. Write for any issues, questions or improvements
